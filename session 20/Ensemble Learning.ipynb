{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=500,\n",
    "                            n_informative=10, n_redundant=20,\n",
    "                           random_state=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "...     X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 500)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=200, oob_score=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=200, oob_score=True)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7909090909090909"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7671641791044777"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04189898156468514,\n",
       " 0.020371816495762217,\n",
       " 0.01768694695638138,\n",
       " 0.014787470033903323,\n",
       " 0.013289686821434362,\n",
       " 0.012550689727084197,\n",
       " 0.012132875994581778,\n",
       " 0.01139996017419799,\n",
       " 0.01107634764486541,\n",
       " 0.010533840022059163,\n",
       " 0.009949878743591422,\n",
       " 0.00989707820469669,\n",
       " 0.008052211098514079,\n",
       " 0.007693984989328645,\n",
       " 0.006975477750899878,\n",
       " 0.006552312518085298,\n",
       " 0.0058021585804997125,\n",
       " 0.005264627906053824,\n",
       " 0.004738301185980468,\n",
       " 0.0046835338043031355,\n",
       " 0.004379986181316959,\n",
       " 0.004196536620934017,\n",
       " 0.0038961106091963084,\n",
       " 0.0037025408296538272,\n",
       " 0.0035825748707756368,\n",
       " 0.003274400547899647,\n",
       " 0.0032583161185926173,\n",
       " 0.0031566180909988395,\n",
       " 0.0030748999458168045,\n",
       " 0.0030571396393248474,\n",
       " 0.00300916206233448,\n",
       " 0.0029856312692700827,\n",
       " 0.0029619231238405337,\n",
       " 0.0029131841943270177,\n",
       " 0.0028673074987375117,\n",
       " 0.0028410654350114577,\n",
       " 0.002821642879095731,\n",
       " 0.0027289697309703727,\n",
       " 0.0027042422235835795,\n",
       " 0.002685999656360909,\n",
       " 0.0026010566548889907,\n",
       " 0.0025656877854591986,\n",
       " 0.0025050477700025568,\n",
       " 0.0024717920993024226,\n",
       " 0.0024546385394701134,\n",
       " 0.002438569039013676,\n",
       " 0.0024262358740215835,\n",
       " 0.0024177214926657505,\n",
       " 0.002390241548506132,\n",
       " 0.0023883202510646294,\n",
       " 0.002379252922849941,\n",
       " 0.002370397679794524,\n",
       " 0.002356053566279566,\n",
       " 0.002324587216090945,\n",
       " 0.0023209474570677767,\n",
       " 0.0023204808278254403,\n",
       " 0.0022872149007341046,\n",
       " 0.0022834143611235257,\n",
       " 0.0022724034000376325,\n",
       " 0.0022658210469187635,\n",
       " 0.0022653895119660204,\n",
       " 0.0022635601606150993,\n",
       " 0.0022520372568958892,\n",
       " 0.002248258766566798,\n",
       " 0.002237042608382819,\n",
       " 0.0022233201553062683,\n",
       " 0.00221780119847686,\n",
       " 0.0021849010646523065,\n",
       " 0.00217988743550715,\n",
       " 0.0021777971897185104,\n",
       " 0.002156722206441177,\n",
       " 0.0021461620372111023,\n",
       " 0.0021453903864235143,\n",
       " 0.002128482886940865,\n",
       " 0.002126673796434776,\n",
       " 0.0021204476196831977,\n",
       " 0.0021058813523672907,\n",
       " 0.002099803863956477,\n",
       " 0.0020938526569612226,\n",
       " 0.002090492605111322,\n",
       " 0.002090141079373804,\n",
       " 0.0020843623050793756,\n",
       " 0.0020805583044689597,\n",
       " 0.0020797017037161984,\n",
       " 0.0020686569321036424,\n",
       " 0.0020655270739327533,\n",
       " 0.002055296810268609,\n",
       " 0.002055178966759723,\n",
       " 0.0020515855166171725,\n",
       " 0.0020470757126716923,\n",
       " 0.002046907595232733,\n",
       " 0.0020414335266759364,\n",
       " 0.0020407398168919354,\n",
       " 0.0020396884326848753,\n",
       " 0.0020368019305061712,\n",
       " 0.002021009171586265,\n",
       " 0.002019930747930827,\n",
       " 0.0020169702796986313,\n",
       " 0.0020168479786010687,\n",
       " 0.0020128241569782996,\n",
       " 0.0020010615662311025,\n",
       " 0.0019994953342281877,\n",
       " 0.001997022283722285,\n",
       " 0.0019953574669346564,\n",
       " 0.0019859411152104976,\n",
       " 0.0019792635702181567,\n",
       " 0.0019789815332202573,\n",
       " 0.0019774011310970028,\n",
       " 0.0019757349046527735,\n",
       " 0.001970294522529142,\n",
       " 0.001969270578750899,\n",
       " 0.0019684158157442194,\n",
       " 0.0019595832120629705,\n",
       " 0.0019550010261667264,\n",
       " 0.0019522995558651879,\n",
       " 0.0019488459322977952,\n",
       " 0.0019437397412943134,\n",
       " 0.0019408882635087283,\n",
       " 0.0019401453246084396,\n",
       " 0.0019380865813471745,\n",
       " 0.0019287242520087994,\n",
       " 0.0019275678710123304,\n",
       " 0.0019177565623289883,\n",
       " 0.0019145661959469092,\n",
       " 0.0018958943505079878,\n",
       " 0.0018953540674624355,\n",
       " 0.0018944464601954236,\n",
       " 0.0018890811718456663,\n",
       " 0.0018854514885394752,\n",
       " 0.0018822088004573781,\n",
       " 0.0018764607052655171,\n",
       " 0.0018743737566553928,\n",
       " 0.0018701445238591488,\n",
       " 0.0018699305999366545,\n",
       " 0.0018673955361600584,\n",
       " 0.0018573305340210391,\n",
       " 0.0018554149454176566,\n",
       " 0.0018553691650016885,\n",
       " 0.0018529726628783174,\n",
       " 0.0018522767515073981,\n",
       " 0.0018418144171597312,\n",
       " 0.0018389486652923422,\n",
       " 0.0018334081693762861,\n",
       " 0.0018297801491266024,\n",
       " 0.0018272254077090992,\n",
       " 0.0018266706904627602,\n",
       " 0.0018260234290886572,\n",
       " 0.0018233403694515398,\n",
       " 0.0018198867028507385,\n",
       " 0.001818586938646267,\n",
       " 0.0018176085518314328,\n",
       " 0.001808789602072556,\n",
       " 0.0018053754626238452,\n",
       " 0.001804281486521349,\n",
       " 0.001803167631815656,\n",
       " 0.0018028550247975153,\n",
       " 0.001798952260522679,\n",
       " 0.0017958566341957536,\n",
       " 0.0017855674908847392,\n",
       " 0.0017852354844484764,\n",
       " 0.0017838364200313001,\n",
       " 0.0017816725030801116,\n",
       " 0.0017758609444237033,\n",
       " 0.001771568640215264,\n",
       " 0.0017708721820596446,\n",
       " 0.0017690837760607442,\n",
       " 0.001768181860793538,\n",
       " 0.0017648554054011728,\n",
       " 0.0017629624099299913,\n",
       " 0.0017622149737420512,\n",
       " 0.0017613930635683344,\n",
       " 0.001760221397785393,\n",
       " 0.0017591741346828344,\n",
       " 0.0017537420525431042,\n",
       " 0.0017515360509993488,\n",
       " 0.0017512474936600758,\n",
       " 0.0017500892424542253,\n",
       " 0.001745346024833376,\n",
       " 0.001742819492669061,\n",
       " 0.0017426548197653539,\n",
       " 0.0017404175823468005,\n",
       " 0.001740282565282366,\n",
       " 0.001738942369709236,\n",
       " 0.00173698051449117,\n",
       " 0.0017300033383127456,\n",
       " 0.0017263936746801686,\n",
       " 0.001725164746133457,\n",
       " 0.0017232222978983862,\n",
       " 0.0017215615181914612,\n",
       " 0.0017201241648870816,\n",
       " 0.0017186109128976147,\n",
       " 0.0017162320954084772,\n",
       " 0.0017161423700256646,\n",
       " 0.0017133120801185197,\n",
       " 0.0017132327795526142,\n",
       " 0.0016969221741690785,\n",
       " 0.001689793996049293,\n",
       " 0.0016869585589987517,\n",
       " 0.0016857598097813368,\n",
       " 0.001683657202518834,\n",
       " 0.0016827771433783375,\n",
       " 0.0016793328672967268,\n",
       " 0.0016751066614288323,\n",
       " 0.0016718870051499707,\n",
       " 0.0016715426835699724,\n",
       " 0.0016707079063822726,\n",
       " 0.0016676735295035668,\n",
       " 0.0016541521591039619,\n",
       " 0.0016512891773201475,\n",
       " 0.0016506864896044212,\n",
       " 0.0016505969982190374,\n",
       " 0.001646469470270795,\n",
       " 0.0016429465043145783,\n",
       " 0.0016418286890888126,\n",
       " 0.001641462375949376,\n",
       " 0.001636917624883883,\n",
       " 0.0016347506534567363,\n",
       " 0.0016316120841424733,\n",
       " 0.001630936884836916,\n",
       " 0.0016304085954502003,\n",
       " 0.0016296150469347305,\n",
       " 0.0016295684529708007,\n",
       " 0.0016277975920146973,\n",
       " 0.001626195332152858,\n",
       " 0.0016227773927352978,\n",
       " 0.001620960664851495,\n",
       " 0.0016171739212176338,\n",
       " 0.0016126226931230511,\n",
       " 0.001610493935885087,\n",
       " 0.001609848529082583,\n",
       " 0.0016095590066824924,\n",
       " 0.0016056647148765982,\n",
       " 0.0016040361493175458,\n",
       " 0.0016014209285334488,\n",
       " 0.0016010517743284837,\n",
       " 0.0016005559512229186,\n",
       " 0.001599670981583009,\n",
       " 0.0015917338132223494,\n",
       " 0.0015876457884841108,\n",
       " 0.0015861451624901105,\n",
       " 0.0015722849291411078,\n",
       " 0.0015643963064070742,\n",
       " 0.0015634688175099616,\n",
       " 0.0015624637579196535,\n",
       " 0.0015619127287807416,\n",
       " 0.0015615447667443592,\n",
       " 0.001561471750920411,\n",
       " 0.0015544218612298186,\n",
       " 0.001553675621644536,\n",
       " 0.0015496988903278446,\n",
       " 0.0015408015780302986,\n",
       " 0.0015400523539145473,\n",
       " 0.001536851787142706,\n",
       " 0.001536463739228808,\n",
       " 0.0015317286575336328,\n",
       " 0.0015316696003614594,\n",
       " 0.0015264393594657025,\n",
       " 0.0015259610109822038,\n",
       " 0.001524226048856876,\n",
       " 0.001523393399521475,\n",
       " 0.0015231744720167887,\n",
       " 0.001521424390704575,\n",
       " 0.0015171538328956535,\n",
       " 0.0015141325695201913,\n",
       " 0.001513844008773351,\n",
       " 0.0015136352219177814,\n",
       " 0.001511249491978717,\n",
       " 0.0015110577664510822,\n",
       " 0.0015101034829864868,\n",
       " 0.0015099995549574142,\n",
       " 0.0015098035496288393,\n",
       " 0.001508717180797835,\n",
       " 0.0015084569025571792,\n",
       " 0.0015058989057601536,\n",
       " 0.0014953233244697553,\n",
       " 0.0014949135256920238,\n",
       " 0.001492654069938553,\n",
       " 0.0014897479799184767,\n",
       " 0.0014893649334163086,\n",
       " 0.0014880052645386704,\n",
       " 0.0014874550971359952,\n",
       " 0.0014862674070620799,\n",
       " 0.001485624117371625,\n",
       " 0.0014830335243826154,\n",
       " 0.0014802446474529898,\n",
       " 0.001479403479961264,\n",
       " 0.0014675669994161922,\n",
       " 0.0014652304652406573,\n",
       " 0.0014611333629718681,\n",
       " 0.0014608520484645005,\n",
       " 0.0014553002164167342,\n",
       " 0.0014532039559070067,\n",
       " 0.0014495537568588385,\n",
       " 0.0014488388064206582,\n",
       " 0.0014432131110044938,\n",
       " 0.0014396794276298676,\n",
       " 0.0014391134579706153,\n",
       " 0.0014374375254794026,\n",
       " 0.001434890247652785,\n",
       " 0.0014341057073866414,\n",
       " 0.0014260638550227273,\n",
       " 0.0014253808127361422,\n",
       " 0.001421163291147647,\n",
       " 0.0014139545286795268,\n",
       " 0.0014128591051077515,\n",
       " 0.0014102354056041752,\n",
       " 0.0014066529752642509,\n",
       " 0.0014022627622696647,\n",
       " 0.001398769238442127,\n",
       " 0.0013979949040395,\n",
       " 0.001396070600829024,\n",
       " 0.001395141468863016,\n",
       " 0.001392910229453959,\n",
       " 0.00139178619154376,\n",
       " 0.0013910714569885982,\n",
       " 0.0013885122016741203,\n",
       " 0.001388490717676394,\n",
       " 0.0013875899003619506,\n",
       " 0.001385714557809985,\n",
       " 0.0013852837502134063,\n",
       " 0.0013821540273329116,\n",
       " 0.0013789090494527255,\n",
       " 0.0013786906152243267,\n",
       " 0.0013766317711050682,\n",
       " 0.0013724811599399422,\n",
       " 0.0013707563217042933,\n",
       " 0.0013706673515068565,\n",
       " 0.0013701885644034656,\n",
       " 0.001369351714152211,\n",
       " 0.0013692303156274544,\n",
       " 0.001368534552972792,\n",
       " 0.001367349590068915,\n",
       " 0.0013579576767128753,\n",
       " 0.0013532808822016502,\n",
       " 0.0013508615006582753,\n",
       " 0.0013508354095123174,\n",
       " 0.0013467464672356094,\n",
       " 0.001344879313859998,\n",
       " 0.0013441544368905996,\n",
       " 0.0013389174270086854,\n",
       " 0.0013375978253219478,\n",
       " 0.0013371676982179162,\n",
       " 0.0013365660835894663,\n",
       " 0.0013348956928409773,\n",
       " 0.0013313617485044805,\n",
       " 0.001325072468690932,\n",
       " 0.0013227826642715085,\n",
       " 0.0013221460042857614,\n",
       " 0.0013181800206867203,\n",
       " 0.0013172171933673136,\n",
       " 0.0013162013718752475,\n",
       " 0.001313792306372459,\n",
       " 0.0013132744456832858,\n",
       " 0.0013122620367506603,\n",
       " 0.0013033499937136163,\n",
       " 0.001299443027312724,\n",
       " 0.0012977133421380486,\n",
       " 0.0012975751216827364,\n",
       " 0.001297420739417878,\n",
       " 0.001292785294131921,\n",
       " 0.0012919514929421764,\n",
       " 0.001289937932634275,\n",
       " 0.0012855482924119832,\n",
       " 0.0012850583204464367,\n",
       " 0.0012838532989389812,\n",
       " 0.0012763044693719489,\n",
       " 0.0012759996660271949,\n",
       " 0.0012742966010569337,\n",
       " 0.001272413203534344,\n",
       " 0.0012673451501445687,\n",
       " 0.0012660613596116423,\n",
       " 0.0012642529922424937,\n",
       " 0.0012626410752629505,\n",
       " 0.0012572420822061694,\n",
       " 0.0012536570591723624,\n",
       " 0.0012525867513434707,\n",
       " 0.0012495269765811546,\n",
       " 0.0012494645541551647,\n",
       " 0.0012472785558973298,\n",
       " 0.0012416732015944406,\n",
       " 0.0012412756294752146,\n",
       " 0.001239971502386342,\n",
       " 0.0012393197399970218,\n",
       " 0.0012365636015613521,\n",
       " 0.001236101093484126,\n",
       " 0.001235384918855261,\n",
       " 0.0012352634632161396,\n",
       " 0.0012321780661940292,\n",
       " 0.0012313625447328736,\n",
       " 0.0012305911621913613,\n",
       " 0.0012250376170573779,\n",
       " 0.0012201601494692541,\n",
       " 0.001219639176369474,\n",
       " 0.001217830708898912,\n",
       " 0.0012041197786062635,\n",
       " 0.00120151956585462,\n",
       " 0.0012008463859666766,\n",
       " 0.0011911098238968861,\n",
       " 0.0011842022409676496,\n",
       " 0.0011822595779971638,\n",
       " 0.0011809715596848179,\n",
       " 0.001179004379999382,\n",
       " 0.001174367737166353,\n",
       " 0.0011739978242642692,\n",
       " 0.0011714502168500046,\n",
       " 0.0011703530571230865,\n",
       " 0.001170073913938961,\n",
       " 0.0011650266599499111,\n",
       " 0.001159020012080792,\n",
       " 0.0011589162691226663,\n",
       " 0.0011535041930851855,\n",
       " 0.0011443213195124263,\n",
       " 0.0011427377978882763,\n",
       " 0.0011396278443778795,\n",
       " 0.0011369884514533246,\n",
       " 0.0011332483058958042,\n",
       " 0.0011293535536227846,\n",
       " 0.0011276138750173869,\n",
       " 0.001126191038690209,\n",
       " 0.0011259495478330866,\n",
       " 0.0011216265918222491,\n",
       " 0.0011197773528478752,\n",
       " 0.0011196684352119206,\n",
       " 0.0011134477620688796,\n",
       " 0.001112043374103788,\n",
       " 0.0011103792727775094,\n",
       " 0.001110095647478456,\n",
       " 0.0011087436147348122,\n",
       " 0.0011063003901024735,\n",
       " 0.0011017081776646437,\n",
       " 0.0010969127244072725,\n",
       " 0.0010958359333793823,\n",
       " 0.0010948770157808273,\n",
       " 0.0010929054599146466,\n",
       " 0.0010863235098510337,\n",
       " 0.0010768264656177025,\n",
       " 0.0010700516285244487,\n",
       " 0.0010629925720951423,\n",
       " 0.0010592484321107958,\n",
       " 0.0010572256183019302,\n",
       " 0.001053352309591687,\n",
       " 0.0010506820745425016,\n",
       " 0.0010490248215245957,\n",
       " 0.0010432574714042005,\n",
       " 0.0010403210496470246,\n",
       " 0.0010385290312528335,\n",
       " 0.0010310384437563543,\n",
       " 0.0010251073253430172,\n",
       " 0.0010154912600516585,\n",
       " 0.0010101288179734033,\n",
       " 0.0010095838476872306,\n",
       " 0.0009959811493130864,\n",
       " 0.0009897103228211732,\n",
       " 0.0009893987681339695,\n",
       " 0.0009788152398921702,\n",
       " 0.0009698609954121094,\n",
       " 0.0009682114408966332,\n",
       " 0.0009606285915620761,\n",
       " 0.0009556830552183415,\n",
       " 0.0009390288097936077,\n",
       " 0.0009374322465264598,\n",
       " 0.0009368191703600376,\n",
       " 0.000933917801493892,\n",
       " 0.0009274242776628882,\n",
       " 0.0009270684818343458,\n",
       " 0.0009244115201616157,\n",
       " 0.0009231772278343477,\n",
       " 0.0009043847482862921,\n",
       " 0.0009023813204562866,\n",
       " 0.0009021646442657712,\n",
       " 0.0008980435494210966,\n",
       " 0.0008881896557073319,\n",
       " 0.0008842454208817896,\n",
       " 0.0008823609925664591,\n",
       " 0.0008820394542358483,\n",
       " 0.0008808695629930029,\n",
       " 0.0008710411754555854,\n",
       " 0.0008702390595556275,\n",
       " 0.0008698499608495185,\n",
       " 0.000868926730294767,\n",
       " 0.0008652884278159174,\n",
       " 0.0008399267503044204,\n",
       " 0.0008220166664406915,\n",
       " 0.0008212457722348616,\n",
       " 0.0008193264480143434,\n",
       " 0.0008073949733644848,\n",
       " 0.0008063658832424251,\n",
       " 0.0008036933435612167,\n",
       " 0.0008011129903549946,\n",
       " 0.0007961008138289014,\n",
       " 0.0007937776316704143,\n",
       " 0.0007923825222720186,\n",
       " 0.00075587260380114,\n",
       " 0.0007440577555177634,\n",
       " 0.0007438370052011981,\n",
       " 0.0007429578703074427,\n",
       " 0.0007417164064392208,\n",
       " 0.000737632041433217,\n",
       " 0.0006611167466060311,\n",
       " 0.0005377378806453579]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature importances in descending order\n",
    "sorted(rf.feature_importances_, reverse= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting top 20 feature indices\n",
    "imp_features = np.argsort(rf.feature_importances_)[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23, 15, 14, 24, 22,  5, 20, 29, 18, 19, 12, 25, 10, 27,  8,  0,  7,\n",
       "        4, 16, 17])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting data of 20 imp. features\n",
    "X_new = X[:, imp_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 20)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "...     X_new, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=200, oob_score=True)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8575757575757575"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = KNeighborsClassifier()\n",
    "m2 = LogisticRegression()\n",
    "m3 = GaussianNB()\n",
    "m4 = DecisionTreeClassifier()\n",
    "m5 = BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VotingClassifier([('knn',m1), ('lr',m2), ('mnb',m3), ('dt',m4),(\n",
    "                            'bnb',m5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('knn', KNeighborsClassifier()),\n",
       "                             ('lr', LogisticRegression()),\n",
       "                             ('mnb', GaussianNB()),\n",
       "                             ('dt', DecisionTreeClassifier()),\n",
       "                             ('bnb', BernoulliNB())])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8462686567164179"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7696969696969697"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "XGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed (vcomp140.dll or libgomp-1.dll for Windows, libomp.dylib for Mac OSX, libgomp.so for Linux and other UNIX-like OSes). Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n  * You are running 32-bit Python on a 64-bit OS\nError message(s): ['dlopen(/Users/mohit/opt/anaconda3/lib/python3.8/site-packages/xgboost/lib/libxgboost.dylib, 6): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\\n  Referenced from: /Users/mohit/opt/anaconda3/lib/python3.8/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: image not found']\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-e528dc1a32ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/xgboost/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDMatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeviceQuantileDMatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBooster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrabit\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;31m# load the XGBoost library globally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m \u001b[0m_LIB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_lib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_load_lib\u001b[0;34m()\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlib_success\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mlibname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlib_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         raise XGBoostError(\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0;34m'XGBoost Library ({}) could not be loaded.\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;34m'Likely causes:\\n'\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXGBoostError\u001b[0m: XGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed (vcomp140.dll or libgomp-1.dll for Windows, libomp.dylib for Mac OSX, libgomp.so for Linux and other UNIX-like OSes). Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n  * You are running 32-bit Python on a 64-bit OS\nError message(s): ['dlopen(/Users/mohit/opt/anaconda3/lib/python3.8/site-packages/xgboost/lib/libxgboost.dylib, 6): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\\n  Referenced from: /Users/mohit/opt/anaconda3/lib/python3.8/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: image not found']\n"
     ]
    }
   ],
   "source": [
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /Users/mohit/opt/anaconda3/lib/python3.8/site-packages (1.3.1)\n",
      "Requirement already satisfied: numpy in /Users/mohit/opt/anaconda3/lib/python3.8/site-packages (from xgboost) (1.18.5)\n",
      "Requirement already satisfied: scipy in /Users/mohit/opt/anaconda3/lib/python3.8/site-packages (from xgboost) (1.5.0)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "XGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed (vcomp140.dll or libgomp-1.dll for Windows, libomp.dylib for Mac OSX, libgomp.so for Linux and other UNIX-like OSes). Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n  * You are running 32-bit Python on a 64-bit OS\nError message(s): ['dlopen(/Users/mohit/opt/anaconda3/lib/python3.8/site-packages/xgboost/lib/libxgboost.dylib, 6): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\\n  Referenced from: /Users/mohit/opt/anaconda3/lib/python3.8/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: image not found']\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-477fa34615c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/xgboost/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDMatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeviceQuantileDMatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBooster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrabit\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;31m# load the XGBoost library globally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m \u001b[0m_LIB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_lib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_load_lib\u001b[0;34m()\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlib_success\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mlibname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlib_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         raise XGBoostError(\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0;34m'XGBoost Library ({}) could not be loaded.\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;34m'Likely causes:\\n'\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXGBoostError\u001b[0m: XGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed (vcomp140.dll or libgomp-1.dll for Windows, libomp.dylib for Mac OSX, libgomp.so for Linux and other UNIX-like OSes). Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n  * You are running 32-bit Python on a 64-bit OS\nError message(s): ['dlopen(/Users/mohit/opt/anaconda3/lib/python3.8/site-packages/xgboost/lib/libxgboost.dylib, 6): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\\n  Referenced from: /Users/mohit/opt/anaconda3/lib/python3.8/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: image not found']\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
